{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/policy-gradient-reinforce-algorithm-with-baseline-e95ace11c1c4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "import tensorflow_probability as tfp\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def export_plot(ys, ylabel, title, filename):\n",
    "    plt.figure()\n",
    "    plt.plot(range(len(ys)), ys)\n",
    "    plt.xlabel(\"Training Episode\")\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.savefig(filename)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNet():\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.model = keras.Sequential(\n",
    "            layers=[\n",
    "                keras.Input(shape=(input_size,)),\n",
    "                layers.Dense(64, activation=\"relu\", name=\"relu_layer\"),\n",
    "                layers.Dense(output_size, activation=\"linear\", name=\"linear_layer\")\n",
    "            ],\n",
    "            name=\"policy\")\n",
    "\n",
    "    def action_distribution(self, observations):\n",
    "        logtis = self.model(observations)\n",
    "        return tfp.distributions.Categorical(logits=logtis) # softmax\n",
    "\n",
    "    def sample_action(self, observartions):\n",
    "        sample_actions = self.action_distribution(observartions).sample().numpy()\n",
    "        return sample_actions\n",
    "\n",
    "    def save_policy_net(self):\n",
    "        self.model.save(\"D:/RL/policy-gradient/results/model\")\n",
    "\n",
    "class BaselineNet():\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.model = keras.Sequential(\n",
    "            layers=[\n",
    "                keras.Input(shape=(input_size,)),\n",
    "                layers.Dense(64, activation=\"relu\", name=\"relu_layer\"),\n",
    "                layers.Dense(output_size, activation=\"linear\", name=\"linear_layer\")\n",
    "            ],\n",
    "            name=\"baseline\")\n",
    "\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=3e-2)\n",
    "\n",
    "    # predict V(s)\n",
    "    def forward(self, observations):\n",
    "        output = tf.squeeze(self.model(observations))\n",
    "        return output\n",
    "\n",
    "    def update(self, observartions, target):\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.forward(observartions)\n",
    "            loss = tf.keras.losses.mean_squared_error(y_true=target, y_pred=predictions)\n",
    "\n",
    "        # backprob\n",
    "        grads = tape.gradient(loss, self.model.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.model.trainable_weights))\n",
    "\n",
    "    def save_baseline_net(self):\n",
    "        self.model.save(\"D:/RL/policy-gradient/results/model\")\n",
    "\n",
    "class PolicyGradient(object):\n",
    "    def __init__(self, env, num_iterations=300, batch_size=2000, max_ep_len=200, output_path=\"../results/\"):\n",
    "        self.output_path = output_path\n",
    "        if not os.path.exists(output_path):\n",
    "            os.makedirs(output_path)\n",
    "\n",
    "        self.env = env\n",
    "        self.observation_dim = self.env.observation_space.shape[0]\n",
    "        self.action_dim = self.env.action_space.n\n",
    "        self.gamma = 0.9\n",
    "        self.num_iterations = num_iterations\n",
    "        self.batch_size = batch_size\n",
    "        self.max_ep_len = max_ep_len\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=3e-2)\n",
    "        self.policy_net = PolicyNet(input_size=self.observation_dim, output_size=self.action_dim) # output action probability distribution\n",
    "        self.baseline_net = BaselineNet(input_size=self.observation_dim, output_size=1) # output V(s)\n",
    "\n",
    "    def play_games(self, env=None, num_episodes=None):\n",
    "        episode = 0\n",
    "        episode_rewards = []\n",
    "        paths = []\n",
    "        t = 0\n",
    "        if not env:\n",
    "            env = self.env\n",
    "\n",
    "        while (num_episodes or t < self.batch_size):\n",
    "            state = env.reset()\n",
    "            states, actions, rewards = [], [], []\n",
    "            episode_reward = 0\n",
    "\n",
    "            for step in range(self.max_ep_len):\n",
    "                states.append(state)\n",
    "                action = self.policy_net.sample_action(np.atleast_2d(state))[0] # creating batch\n",
    "                state, reward, done, _ = env.step(action)\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                episode_reward += reward\n",
    "                t += 1\n",
    "\n",
    "                if (done or step == self.max_ep_len-1):\n",
    "                    episode_rewards.append(episode_reward)\n",
    "                    break\n",
    "                if (not num_episodes) and t == self.batch_size:\n",
    "                    break\n",
    "\n",
    "            # each episode's path\n",
    "            path = {\"observation\": np.array(states),\n",
    "                    \"reward\": np.array(rewards),\n",
    "                    \"action\": np.array(actions)}\n",
    "            paths.append(path)\n",
    "            episode += 1\n",
    "            if num_episodes and episode >= num_episodes:\n",
    "                break\n",
    "\n",
    "        return paths, episode_rewards # paths is the transitions of all episodes\n",
    "\n",
    "    def get_returns(self, paths):\n",
    "        all_returns = []\n",
    "        for path in paths:\n",
    "            rewards = path[\"reward\"] # a list of rewards in an episode\n",
    "            returns = []\n",
    "            reversed_rewards = np.flip(rewards,0)\n",
    "            g_t = 0\n",
    "\n",
    "            # discounted return of an episode\n",
    "            for r in reversed_rewards:\n",
    "                g_t = r + self.gamma * g_t\n",
    "                returns.insert(0, g_t)\n",
    "\n",
    "            # discounted return of all episodes\n",
    "            all_returns.append(returns)\n",
    "\n",
    "        returns = np.concatenate(all_returns)\n",
    "        return returns\n",
    "\n",
    "    def get_advantage(self, returns, observations):\n",
    "        values = self.baseline_net.forward(observations).numpy() # calculate b\n",
    "        advantages = returns - values # G - b\n",
    "\n",
    "        # normalize\n",
    "        advantages = (advantages-np.mean(advantages)) / np.sqrt(np.sum(advantages**2))\n",
    "        return advantages\n",
    "\n",
    "    def update_policy(self, observations, actions, advantages):\n",
    "        observations = tf.convert_to_tensor(observations)\n",
    "        actions = tf.convert_to_tensor(actions)\n",
    "        advantages = tf.convert_to_tensor(advantages)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            log_prob = self.policy_net.action_distribution(observations).log_prob(actions)\n",
    "            loss = -tf.math.reduce_mean(log_prob * tf.cast(advantages, tf.float32)) # mean of batch\n",
    "\n",
    "        # backprob\n",
    "        grads = tape.gradient(loss, self.policy_net.model.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.policy_net.model.trainable_weights))\n",
    "\n",
    "    def train(self):\n",
    "        all_total_rewards = []\n",
    "        averaged_total_rewards = []\n",
    "\n",
    "        for t in range(self.num_iterations):\n",
    "            paths, total_rewards = self.play_games()\n",
    "            all_total_rewards.extend(total_rewards)\n",
    "            observations = np.concatenate([path[\"observation\"] for path in paths])\n",
    "            actions = np.concatenate([path[\"action\"] for path in paths])\n",
    "            returns = self.get_returns(paths)\n",
    "            advantages = self.get_advantage(returns, observations)\n",
    "            self.baseline_net.update(observations, returns)\n",
    "            self.update_policy(observations, actions, advantages)\n",
    "            avg_reward = np.mean(total_rewards)\n",
    "            averaged_total_rewards.append(avg_reward)\n",
    "            print(\"Average reward for batch {}: {:04.2f}\".format(t,avg_reward))\n",
    "\n",
    "        print(\"Training complete\")\n",
    "        np.save(self.output_path+ \"rewards.npy\", averaged_total_rewards)\n",
    "        export_plot(averaged_total_rewards, \"Reward\", \"CartPole-v0\", self.output_path + \"rewards.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward for batch 0: 18.67\n",
      "Average reward for batch 1: 34.31\n",
      "Average reward for batch 2: 40.55\n",
      "Average reward for batch 3: 50.90\n",
      "Average reward for batch 4: 59.82\n",
      "Average reward for batch 5: 72.37\n",
      "Average reward for batch 6: 96.70\n",
      "Average reward for batch 7: 98.60\n",
      "Average reward for batch 8: 121.38\n",
      "Average reward for batch 9: 142.07\n",
      "Average reward for batch 10: 161.08\n",
      "Average reward for batch 11: 180.50\n",
      "Average reward for batch 12: 190.60\n",
      "Average reward for batch 13: 188.20\n",
      "Average reward for batch 14: 198.40\n",
      "Average reward for batch 15: 199.60\n",
      "Average reward for batch 16: 164.83\n",
      "Average reward for batch 17: 136.07\n",
      "Average reward for batch 18: 114.29\n",
      "Average reward for batch 19: 104.95\n",
      "Average reward for batch 20: 74.77\n",
      "Average reward for batch 21: 61.34\n",
      "Average reward for batch 22: 79.32\n",
      "Average reward for batch 23: 110.94\n",
      "Average reward for batch 24: 129.27\n",
      "Average reward for batch 25: 163.25\n",
      "Average reward for batch 26: 184.40\n",
      "Average reward for batch 27: 197.50\n",
      "Average reward for batch 28: 200.00\n",
      "Average reward for batch 29: 200.00\n",
      "Average reward for batch 30: 200.00\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "model = PolicyGradient(env)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action = model.policy_net.sample_action(np.atleast_2d(state))[0]\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    env.render()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: D:/RL/policy-gradient/results/model\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: D:/RL/policy-gradient/results/model\\assets\n"
     ]
    }
   ],
   "source": [
    "model.policy_net.save_policy_net()\n",
    "model.baseline_net.save_baseline_net()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
