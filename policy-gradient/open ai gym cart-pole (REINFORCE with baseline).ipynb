{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/policy-gradient-reinforce-algorithm-with-baseline-e95ace11c1c4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "import tensorflow_probability as tfp\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def export_plot(ys, ylabel, title, filename):\n",
    "    plt.figure()\n",
    "    plt.plot(range(len(ys)), ys)\n",
    "    plt.xlabel(\"Training Episode\")\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.savefig(filename)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNet():\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.model = keras.Sequential(\n",
    "            layers=[\n",
    "                keras.Input(shape=(input_size,)),\n",
    "                layers.Dense(64, activation=\"relu\", name=\"relu_layer\"),\n",
    "                layers.Dense(output_size, activation=\"linear\", name=\"linear_layer\")\n",
    "            ],\n",
    "            name=\"policy\")\n",
    "\n",
    "    def action_distribution(self, observations):\n",
    "        logtis = self.model(observations)\n",
    "        return tfp.distributions.Categorical(logits=logtis) # softmax\n",
    "\n",
    "    def sample_action(self, observartions):\n",
    "        sample_actions = self.action_distribution(observartions).sample().numpy()\n",
    "        return sample_actions\n",
    "\n",
    "    def save_policy_net(self):\n",
    "        self.model.save(\"D:\\RL\\policy-gradient\\results\")\n",
    "\n",
    "class BaselineNet():\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.model = keras.Sequential(\n",
    "            layers=[\n",
    "                keras.Input(shape=(input_size,)),\n",
    "                layers.Dense(64, activation=\"relu\", name=\"relu_layer\"),\n",
    "                layers.Dense(output_size, activation=\"linear\", name=\"linear_layer\")\n",
    "            ],\n",
    "            name=\"baseline\")\n",
    "\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=3e-2)\n",
    "\n",
    "    # predict V(s)\n",
    "    def forward(self, observations):\n",
    "        output = tf.squeeze(self.model(observations))\n",
    "        return output\n",
    "\n",
    "    def update(self, observartions, target):\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.forward(observartions)\n",
    "            loss = tf.keras.losses.mean_squared_error(y_true=target, y_pred=predictions)\n",
    "\n",
    "        # backprob\n",
    "        grads = tape.gradient(loss, self.model.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.model.trainable_weights))\n",
    "\n",
    "    def save_baseline_net(self):\n",
    "        self.model.save(\"D:\\RL\\policy-gradient\\results\")\n",
    "\n",
    "class PolicyGradient(object):\n",
    "    def __init__(self, env, num_iterations=300, batch_size=2000, max_ep_len=200, output_path=\"../results/\"):\n",
    "        self.output_path = output_path\n",
    "        if not os.path.exists(output_path):\n",
    "            os.makedirs(output_path)\n",
    "\n",
    "        self.env = env\n",
    "        self.observation_dim = self.env.observation_space.shape[0]\n",
    "        self.action_dim = self.env.action_space.n\n",
    "        self.gamma = 0.9\n",
    "        self.num_iterations = num_iterations\n",
    "        self.batch_size = batch_size\n",
    "        self.max_ep_len = max_ep_len\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=3e-2)\n",
    "        self.policy_net = PolicyNet(input_size=self.observation_dim, output_size=self.action_dim) # output action probability distribution\n",
    "        self.baseline_net = BaselineNet(input_size=self.observation_dim, output_size=1) # output V(s)\n",
    "\n",
    "    def play_games(self, env=None, num_episodes=None):\n",
    "        episode = 0\n",
    "        episode_rewards = []\n",
    "        paths = []\n",
    "        t = 0\n",
    "        if not env:\n",
    "            env = self.env\n",
    "\n",
    "        while (num_episodes or t < self.batch_size):\n",
    "            state = env.reset()\n",
    "            states, actions, rewards = [], [], []\n",
    "            episode_reward = 0\n",
    "\n",
    "            for step in range(self.max_ep_len):\n",
    "                states.append(state)\n",
    "                action = self.policy_net.sample_action(np.atleast_2d(state))[0] # creating batch\n",
    "                state, reward, done, _ = env.step(action)\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                episode_reward += reward\n",
    "                t += 1\n",
    "\n",
    "                if (done or step == self.max_ep_len-1):\n",
    "                    episode_rewards.append(episode_reward)\n",
    "                    break\n",
    "                if (not num_episodes) and t == self.batch_size:\n",
    "                    break\n",
    "\n",
    "            # each episode's path\n",
    "            path = {\"observation\": np.array(states),\n",
    "                    \"reward\": np.array(rewards),\n",
    "                    \"action\": np.array(actions)}\n",
    "            paths.append(path)\n",
    "            episode += 1\n",
    "            if num_episodes and episode >= num_episodes:\n",
    "                break\n",
    "\n",
    "        return paths, episode_rewards # paths is the transitions of all episodes\n",
    "\n",
    "    def get_returns(self, paths):\n",
    "        all_returns = []\n",
    "        for path in paths:\n",
    "            rewards = path[\"reward\"] # a list of rewards in an episode\n",
    "            returns = []\n",
    "            reversed_rewards = np.flip(rewards,0)\n",
    "            g_t = 0\n",
    "\n",
    "            # discounted return of an episode\n",
    "            for r in reversed_rewards:\n",
    "                g_t = r + self.gamma * g_t\n",
    "                returns.insert(0, g_t)\n",
    "\n",
    "            # discounted return of all episodes\n",
    "            all_returns.append(returns)\n",
    "\n",
    "        returns = np.concatenate(all_returns)\n",
    "        return returns\n",
    "\n",
    "    def get_advantage(self, returns, observations):\n",
    "        values = self.baseline_net.forward(observations).numpy() # calculate b\n",
    "        advantages = returns - values # G - b\n",
    "\n",
    "        # normalize\n",
    "        advantages = (advantages-np.mean(advantages)) / np.sqrt(np.sum(advantages**2))\n",
    "        return advantages\n",
    "\n",
    "    def update_policy(self, observations, actions, advantages):\n",
    "        observations = tf.convert_to_tensor(observations)\n",
    "        actions = tf.convert_to_tensor(actions)\n",
    "        advantages = tf.convert_to_tensor(advantages)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            log_prob = self.policy_net.action_distribution(observations).log_prob(actions)\n",
    "            loss = -tf.math.reduce_mean(log_prob * tf.cast(advantages, tf.float32)) # mean of batch\n",
    "\n",
    "        # backprob\n",
    "        grads = tape.gradient(loss, self.policy_net.model.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.policy_net.model.trainable_weights))\n",
    "\n",
    "    def train(self):\n",
    "        all_total_rewards = []\n",
    "        averaged_total_rewards = []\n",
    "\n",
    "        for t in range(self.num_iterations):\n",
    "            paths, total_rewards = self.play_games()\n",
    "            all_total_rewards.extend(total_rewards)\n",
    "            observations = np.concatenate([path[\"observation\"] for path in paths])\n",
    "            actions = np.concatenate([path[\"action\"] for path in paths])\n",
    "            returns = self.get_returns(paths)\n",
    "            advantages = self.get_advantage(returns, observations)\n",
    "            self.baseline_net.update(observations, returns)\n",
    "            self.update_policy(observations, actions, advantages)\n",
    "            avg_reward = np.mean(total_rewards)\n",
    "            averaged_total_rewards.append(avg_reward)\n",
    "            print(\"Average reward for batch {}: {:04.2f}\".format(t,avg_reward))\n",
    "\n",
    "        print(\"Training complete\")\n",
    "        np.save(self.output_path+ \"rewards.npy\", averaged_total_rewards)\n",
    "        export_plot(averaged_total_rewards, \"Reward\", \"CartPole-v0\", self.output_path + \"rewards.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward for batch 0: 18.64\n",
      "Average reward for batch 1: 31.22\n",
      "Average reward for batch 2: 44.09\n",
      "Average reward for batch 3: 52.16\n",
      "Average reward for batch 4: 64.19\n",
      "Average reward for batch 5: 72.11\n",
      "Average reward for batch 6: 142.08\n",
      "Average reward for batch 7: 144.85\n",
      "Average reward for batch 8: 181.36\n",
      "Average reward for batch 9: 185.30\n",
      "Average reward for batch 10: 173.82\n",
      "Average reward for batch 11: 141.69\n",
      "Average reward for batch 12: 164.17\n",
      "Average reward for batch 13: 196.50\n",
      "Average reward for batch 14: 191.00\n",
      "Average reward for batch 15: 196.60\n",
      "Average reward for batch 16: 195.50\n",
      "Average reward for batch 17: 180.36\n",
      "Average reward for batch 18: 200.00\n",
      "Average reward for batch 19: 176.18\n",
      "Average reward for batch 20: 197.00\n",
      "Average reward for batch 21: 193.50\n",
      "Average reward for batch 22: 194.80\n",
      "Average reward for batch 23: 168.82\n",
      "Average reward for batch 24: 192.70\n",
      "Average reward for batch 25: 193.90\n",
      "Average reward for batch 26: 200.00\n",
      "Average reward for batch 27: 200.00\n",
      "Average reward for batch 28: 181.45\n",
      "Average reward for batch 29: 195.30\n",
      "Average reward for batch 30: 191.90\n",
      "Average reward for batch 31: 198.60\n",
      "Average reward for batch 32: 200.00\n",
      "Average reward for batch 33: 196.00\n",
      "Average reward for batch 34: 166.67\n",
      "Average reward for batch 35: 160.58\n",
      "Average reward for batch 36: 144.08\n",
      "Average reward for batch 37: 126.60\n",
      "Average reward for batch 38: 128.00\n",
      "Average reward for batch 39: 109.33\n",
      "Average reward for batch 40: 104.95\n",
      "Average reward for batch 41: 94.14\n",
      "Average reward for batch 42: 114.53\n",
      "Average reward for batch 43: 125.40\n",
      "Average reward for batch 44: 128.27\n",
      "Average reward for batch 45: 133.86\n",
      "Average reward for batch 46: 135.43\n",
      "Average reward for batch 47: 140.43\n",
      "Average reward for batch 48: 144.46\n",
      "Average reward for batch 49: 148.31\n",
      "Average reward for batch 50: 145.54\n",
      "Average reward for batch 51: 154.67\n",
      "Average reward for batch 52: 163.67\n",
      "Average reward for batch 53: 168.09\n",
      "Average reward for batch 54: 171.18\n",
      "Average reward for batch 55: 161.42\n",
      "Average reward for batch 56: 152.00\n",
      "Average reward for batch 57: 140.43\n",
      "Average reward for batch 58: 138.00\n",
      "Average reward for batch 59: 129.80\n",
      "Average reward for batch 60: 127.20\n",
      "Average reward for batch 61: 119.31\n",
      "Average reward for batch 62: 108.89\n",
      "Average reward for batch 63: 109.06\n",
      "Average reward for batch 64: 119.31\n",
      "Average reward for batch 65: 138.14\n",
      "Average reward for batch 66: 160.08\n",
      "Average reward for batch 67: 172.27\n",
      "Average reward for batch 68: 182.90\n",
      "Average reward for batch 69: 184.30\n",
      "Average reward for batch 70: 198.50\n",
      "Average reward for batch 71: 197.30\n",
      "Average reward for batch 72: 198.50\n",
      "Average reward for batch 73: 191.40\n",
      "Average reward for batch 74: 172.55\n",
      "Average reward for batch 75: 157.92\n",
      "Average reward for batch 76: 149.54\n",
      "Average reward for batch 77: 154.50\n",
      "Average reward for batch 78: 142.43\n",
      "Average reward for batch 79: 133.07\n",
      "Average reward for batch 80: 122.06\n",
      "Average reward for batch 81: 116.47\n",
      "Average reward for batch 82: 109.11\n",
      "Average reward for batch 83: 97.20\n",
      "Average reward for batch 84: 89.68\n",
      "Average reward for batch 85: 101.00\n",
      "Average reward for batch 86: 104.11\n",
      "Average reward for batch 87: 94.52\n",
      "Average reward for batch 88: 104.42\n",
      "Average reward for batch 89: 110.67\n",
      "Average reward for batch 90: 114.88\n",
      "Average reward for batch 91: 119.88\n",
      "Average reward for batch 92: 118.06\n",
      "Average reward for batch 93: 120.06\n",
      "Average reward for batch 94: 123.31\n",
      "Average reward for batch 95: 126.00\n",
      "Average reward for batch 96: 126.20\n",
      "Average reward for batch 97: 126.53\n",
      "Average reward for batch 98: 132.47\n",
      "Average reward for batch 99: 136.21\n",
      "Average reward for batch 100: 138.21\n",
      "Average reward for batch 101: 138.64\n",
      "Average reward for batch 102: 142.79\n",
      "Average reward for batch 103: 143.85\n",
      "Average reward for batch 104: 150.15\n",
      "Average reward for batch 105: 148.92\n",
      "Average reward for batch 106: 144.31\n",
      "Average reward for batch 107: 136.00\n",
      "Average reward for batch 108: 128.80\n",
      "Average reward for batch 109: 127.07\n",
      "Average reward for batch 110: 126.33\n",
      "Average reward for batch 111: 126.47\n",
      "Average reward for batch 112: 129.60\n",
      "Average reward for batch 113: 135.07\n",
      "Average reward for batch 114: 141.79\n",
      "Average reward for batch 115: 146.38\n",
      "Average reward for batch 116: 146.15\n",
      "Average reward for batch 117: 142.77\n",
      "Average reward for batch 118: 146.62\n",
      "Average reward for batch 119: 146.46\n",
      "Average reward for batch 120: 144.69\n",
      "Average reward for batch 121: 147.77\n",
      "Average reward for batch 122: 153.00\n",
      "Average reward for batch 123: 156.08\n",
      "Average reward for batch 124: 162.92\n",
      "Average reward for batch 125: 164.83\n",
      "Average reward for batch 126: 170.91\n",
      "Average reward for batch 127: 165.17\n",
      "Average reward for batch 128: 151.54\n",
      "Average reward for batch 129: 140.36\n",
      "Average reward for batch 130: 139.07\n",
      "Average reward for batch 131: 133.36\n",
      "Average reward for batch 132: 132.13\n",
      "Average reward for batch 133: 130.87\n",
      "Average reward for batch 134: 129.27\n",
      "Average reward for batch 135: 131.40\n",
      "Average reward for batch 136: 134.07\n",
      "Average reward for batch 137: 133.07\n",
      "Average reward for batch 138: 136.93\n",
      "Average reward for batch 139: 137.00\n",
      "Average reward for batch 140: 138.64\n",
      "Average reward for batch 141: 133.00\n",
      "Average reward for batch 142: 133.36\n",
      "Average reward for batch 143: 130.53\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "model = PolicyGradient(env)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action = model.policy_net.sample_action(np.atleast_2d(state))[0]\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    env.render()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PolicyNet' object has no attribute 'save_policy_net'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2640\\493167883.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicy_net\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_policy_net\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbaseline_net\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_baseline_net\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'PolicyNet' object has no attribute 'save_policy_net'"
     ]
    }
   ],
   "source": [
    "model.policy_net.save_policy_net()\n",
    "model.baseline_net.save_baseline_net()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
