{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/policy-gradient-reinforce-algorithm-with-baseline-e95ace11c1c4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "import tensorflow_probability as tfp\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def export_plot(ys, ylabel, title, filename):\n",
    "    plt.figure()\n",
    "    plt.plot(range(len(ys)), ys)\n",
    "    plt.xlabel(\"Training Episode\")\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.savefig(filename)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNet():\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.model = keras.Sequential(\n",
    "            layers=[\n",
    "                keras.Input(shape=(input_size,)),\n",
    "                layers.Dense(64, activation=\"relu\", name=\"relu_layer\"),\n",
    "                layers.Dense(output_size, activation=\"linear\", name=\"linear_layer\")\n",
    "            ],\n",
    "            name=\"policy\")\n",
    "\n",
    "    def action_distribution(self, observations):\n",
    "        logtis = self.model(observations)\n",
    "        return tfp.distributions.Categorical(logits=logtis) # softmax\n",
    "\n",
    "    def sample_action(self, observartions):\n",
    "        sample_actions = self.action_distribution(observartions).sample().numpy()\n",
    "        return sample_actions\n",
    "\n",
    "class BaselineNet():\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.model = keras.Sequential(\n",
    "            layers=[\n",
    "                keras.Input(shape=(input_size,)),\n",
    "                layers.Dense(64, activation=\"relu\", name=\"relu_layer\"),\n",
    "                layers.Dense(output_size, activation=\"linear\", name=\"linear_layer\")\n",
    "            ],\n",
    "            name=\"baseline\")\n",
    "\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=3e-2)\n",
    "\n",
    "    # predict V(s)\n",
    "    def forward(self, observations):\n",
    "        output = tf.squeeze(self.model(observations))\n",
    "        return output\n",
    "\n",
    "    def update(self, observartions, target):\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.forward(observartions)\n",
    "            loss = tf.keras.losses.mean_squared_error(y_true=target, y_pred=predictions)\n",
    "\n",
    "        # backprob\n",
    "        grads = tape.gradient(loss, self.model.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.model.trainable_weights))\n",
    "\n",
    "class PolicyGradient(object):\n",
    "    def __init__(self, env, num_iterations=300, batch_size=2000, max_ep_len=200, output_path=\"../results/\"):\n",
    "        self.output_path = output_path\n",
    "        if not os.path.exists(output_path):\n",
    "            os.makedirs(output_path)\n",
    "\n",
    "        self.env = env\n",
    "        self.observation_dim = self.env.observation_space.shape[0]\n",
    "        self.action_dim = self.env.action_space.n\n",
    "        self.gamma = 0.9\n",
    "        self.num_iterations = num_iterations\n",
    "        self.batch_size = batch_size\n",
    "        self.max_ep_len = max_ep_len\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=3e-2)\n",
    "        self.policy_net = PolicyNet(input_size=self.observation_dim, output_size=self.action_dim) # output action probability distribution\n",
    "        self.baseline_net = BaselineNet(input_size=self.observation_dim, output_size=1) # output V(s)\n",
    "\n",
    "    def play_games(self, env=None, num_episodes=None):\n",
    "        episode = 0\n",
    "        episode_rewards = []\n",
    "        paths = []\n",
    "        t = 0\n",
    "        if not env:\n",
    "            env = self.env\n",
    "\n",
    "        while (num_episodes or t < self.batch_size):\n",
    "            state = env.reset()\n",
    "            states, actions, rewards = [], [], []\n",
    "            episode_reward = 0\n",
    "\n",
    "            for step in range(self.max_ep_len):\n",
    "                states.append(state)\n",
    "                action = self.policy_net.sample_action(np.atleast_2d(state))[0] # creating batch\n",
    "                state, reward, done, _ = env.step(action)\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                episode_reward += reward\n",
    "                t += 1\n",
    "\n",
    "                if (done or step == self.max_ep_len-1):\n",
    "                    episode_rewards.append(episode_reward)\n",
    "                    break\n",
    "                if (not num_episodes) and t == self.batch_size:\n",
    "                    break\n",
    "\n",
    "            # each episode's path\n",
    "            path = {\"observation\": np.array(states),\n",
    "                    \"reward\": np.array(rewards),\n",
    "                    \"action\": np.array(actions)}\n",
    "            paths.append(path)\n",
    "            episode += 1\n",
    "            if num_episodes and episode >= num_episodes:\n",
    "                break\n",
    "\n",
    "        return paths, episode_rewards # paths is the transitions of all episodes\n",
    "\n",
    "    def get_returns(self, paths):\n",
    "        all_returns = []\n",
    "        for path in paths:\n",
    "            rewards = path[\"reward\"] # a list of rewards in an episode\n",
    "            returns = []\n",
    "            reversed_rewards = np.flip(rewards,0)\n",
    "            g_t = 0\n",
    "\n",
    "            # discounted return of an episode\n",
    "            for r in reversed_rewards:\n",
    "                g_t = r + self.gamma * g_t\n",
    "                returns.insert(0, g_t)\n",
    "\n",
    "            # discounted return of all episodes\n",
    "            all_returns.append(returns)\n",
    "\n",
    "        returns = np.concatenate(all_returns)\n",
    "        return returns\n",
    "\n",
    "    def get_advantage(self, returns, observations):\n",
    "        values = self.baseline_net.forward(observations).numpy() # calculate b\n",
    "        advantages = returns - values # G - b\n",
    "\n",
    "        # normalize\n",
    "        advantages = (advantages-np.mean(advantages)) / np.sqrt(np.sum(advantages**2))\n",
    "        return advantages\n",
    "\n",
    "    def update_policy(self, observations, actions, advantages):\n",
    "        observations = tf.convert_to_tensor(observations)\n",
    "        actions = tf.convert_to_tensor(actions)\n",
    "        advantages = tf.convert_to_tensor(advantages)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            log_prob = self.policy_net.action_distribution(observations).log_prob(actions)\n",
    "            loss = -tf.math.reduce_mean(log_prob * tf.cast(advantages, tf.float32)) # mean of batch\n",
    "\n",
    "        # backprob\n",
    "        grads = tape.gradient(loss, self.policy_net.model.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.policy_net.model.trainable_weights))\n",
    "\n",
    "    def train(self):\n",
    "        all_total_rewards = []\n",
    "        averaged_total_rewards = []\n",
    "\n",
    "        for t in range(self.num_iterations):\n",
    "            paths, total_rewards = self.play_games()\n",
    "            all_total_rewards.extend(total_rewards)\n",
    "            observations = np.concatenate([path[\"observation\"] for path in paths])\n",
    "            actions = np.concatenate([path[\"action\"] for path in paths])\n",
    "            returns = self.get_returns(paths)\n",
    "            advantages = self.get_advantage(returns, observations)\n",
    "            self.baseline_net.update(observations, returns)\n",
    "            self.update_policy(observations, actions, advantages)\n",
    "            avg_reward = np.mean(total_rewards)\n",
    "            averaged_total_rewards.append(avg_reward)\n",
    "            print(\"Average reward for batch {}: {:04.2f}\".format(t,avg_reward))\n",
    "\n",
    "        print(\"Training complete\")\n",
    "        np.save(self.output_path+ \"rewards.npy\", averaged_total_rewards)\n",
    "        export_plot(averaged_total_rewards, \"Reward\", \"CartPole-v0\", self.output_path + \"rewards.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "model = PolicyGradient(env)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# old agent play game\n",
    "state = env.reset()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action = model.policy_net.sample_action(np.atleast_2d(state))[0]\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    env.render()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save old agent\n",
    "PATH = \"D:/RL/policy-gradient/results/h5_weights.h5\"\n",
    "model.policy_net.model.save_weights(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new agent\n",
    "PATH = \"D:/RL/policy-gradient/results/h5_weights.h5\"\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "new_policy_net = PolicyNet(input_size=env.observation_space.shape[0], output_size=env.action_space.n)\n",
    "new_policy_net.model.load_weights(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new agent play game\n",
    "state = env.reset()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action = new_policy_net.sample_action(np.atleast_2d(state))[0]\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    env.render()\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
