{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.sefidian.com/2021/03/01/policy-g/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\RL\\policy-gradient\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import gym\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_size = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "HIDDEN_SIZE = 256\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(obs_size, HIDDEN_SIZE),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(HIDDEN_SIZE, n_actions),\n",
    "    torch.nn.Softmax(dim=0) # turns logits to probability\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trajectory 100\tAverage Score: 185.59\n",
      "Trajectory 200\tAverage Score: 176.88\n",
      "Trajectory 300\tAverage Score: 195.61\n",
      "Trajectory 400\tAverage Score: 194.49\n",
      "Trajectory 500\tAverage Score: 193.33\n",
      "Trajectory 600\tAverage Score: 183.76\n",
      "Trajectory 700\tAverage Score: 196.63\n",
      "Trajectory 800\tAverage Score: 182.41\n",
      "Trajectory 900\tAverage Score: 185.27\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.003\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    " \n",
    "Horizon = 500\n",
    "MAX_TRAJECTORIES = 1000\n",
    "gamma = 0.99\n",
    "score = []\n",
    "\n",
    "for trajectory in range(MAX_TRAJECTORIES):\n",
    "    curr_state = env.reset()\n",
    "    done = False\n",
    "    transitions = []\n",
    "\n",
    "    for t in range(Horizon):\n",
    "        act_prob = model(torch.from_numpy(curr_state).float())\n",
    "        action = np.random.choice(np.array([0,1]), p=act_prob.data.numpy()) # choose action according to probability\n",
    "        prev_state = curr_state\n",
    "        curr_state, _, done, info = env.step(action)\n",
    "        transitions.append((prev_state, action, t+1)) # define reward ourselves. the longer time goes on, the more reward\n",
    "        if done:\n",
    "            break\n",
    "    score.append(len(transitions)) #  keep track of the trajectory length over training time . score should be greater as training goes on\n",
    "    reward_batch = torch.tensor([r for (s,a,r) in transitions]).flip(dims=(0,)) # reward for each episode in reverse order\n",
    "\n",
    "    batch_Gvals = []\n",
    "    for i in range(len(transitions)): # len(transitions) is the steps in each episode\n",
    "                                      # for each of the transitions, calculate expected return\n",
    "        new_Gval = 0\n",
    "        power = 0\n",
    "        for j in range(i, len(transitions)): # calculate expected return for each transition\n",
    "            new_Gval=new_Gval+((gamma**power)*reward_batch[j]).numpy() # as j increase, the fewer the expected reward (because time has passed and we expect the episode to terminate)\n",
    "            power += 1\n",
    "        batch_Gvals.append(new_Gval)\n",
    "\n",
    "    # normalize\n",
    "    # expected_returns_batch stores expected returns for all the transitions(step) of the current episode\n",
    "    # expected_returns_batch = R(tau) = (G_0, G_1, G_k) where tau represents current trajectory(episode) and k represent each transition(step)\n",
    "    expected_returns_batch=torch.FloatTensor(batch_Gvals)\n",
    "    expected_returns_batch /= expected_returns_batch.max()\n",
    "\n",
    "    state_batch = torch.Tensor([s for (s,a,r) in transitions])\n",
    "    action_batch = torch.Tensor([a for (s,a,r) in transitions])\n",
    "\n",
    "    # group the action probabilities associated with the actions that were taken\n",
    "    pred_batch = model(state_batch)    \n",
    "    prob_batch = pred_batch.gather(dim=1,index=action_batch.long().view(-1,1)).squeeze()\n",
    "\n",
    "    loss = - torch.sum(torch.log(prob_batch) * expected_returns_batch)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step() \n",
    "\n",
    "    if trajectory % 100 == 0 and trajectory>0:\n",
    "            print('Trajectory {}\\tAverage Score: {:.2f}'.format(trajectory, np.mean(score[-50:-1])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    act_prob = model(torch.from_numpy(state).float())\n",
    "    action = np.random.choice(np.array([0,1]), p=act_prob.data.numpy()) # choose action according to probability\n",
    "    state, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
