{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/policy-gradient-reinforce-algorithm-with-baseline-e95ace11c1c4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "import tensorflow_probability as tfp\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def export_plot(ys, ylabel, title, filename):\n",
    "    plt.figure()\n",
    "    plt.plot(range(len(ys)), ys)\n",
    "    plt.xlabel(\"Training Episode\")\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.savefig(filename)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNet():\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.model = keras.Sequential(\n",
    "            layers=[\n",
    "                keras.Input(shape=(input_size,)),\n",
    "                layers.Dense(64, activation=\"relu\", name=\"relu_layer\"),\n",
    "                layers.Dense(output_size, activation=\"linear\", name=\"linear_layer\")\n",
    "            ],\n",
    "            name=\"policy\")\n",
    "\n",
    "    def action_distribution(self, observations):\n",
    "        logtis = self.model(observations)\n",
    "        return tfp.distributions.Categorical(logits=logtis) # softmax\n",
    "\n",
    "    def sample_action(self, observartions):\n",
    "        sample_actions = self.action_distribution(observartions).sample().numpy()\n",
    "        return sample_actions\n",
    "\n",
    "    def save_policy_net(self):\n",
    "        self.model.save(\"D:/RL/policy-gradient/results/model\")\n",
    "        self.model.save(\"D:/RL/policy-gradient/results/h5file.h5\")\n",
    "        self.model.save(\"D:/RL/policy-gradient/results/h5_weights.h5\")\n",
    "\n",
    "class BaselineNet():\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.model = keras.Sequential(\n",
    "            layers=[\n",
    "                keras.Input(shape=(input_size,)),\n",
    "                layers.Dense(64, activation=\"relu\", name=\"relu_layer\"),\n",
    "                layers.Dense(output_size, activation=\"linear\", name=\"linear_layer\")\n",
    "            ],\n",
    "            name=\"baseline\")\n",
    "\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=3e-2)\n",
    "\n",
    "    # predict V(s)\n",
    "    def forward(self, observations):\n",
    "        output = tf.squeeze(self.model(observations))\n",
    "        return output\n",
    "\n",
    "    def update(self, observartions, target):\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.forward(observartions)\n",
    "            loss = tf.keras.losses.mean_squared_error(y_true=target, y_pred=predictions)\n",
    "\n",
    "        # backprob\n",
    "        grads = tape.gradient(loss, self.model.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.model.trainable_weights))\n",
    "\n",
    "    def save_baseline_net(self):\n",
    "        self.model.save(\"D:/RL/policy-gradient/results/model\")\n",
    "        self.model.save(\"D:/RL/policy-gradient/results/h5file.h5\")\n",
    "        self.model.save(\"D:/RL/policy-gradient/results/h5_weights.h5\")\n",
    "\n",
    "class PolicyGradient(object):\n",
    "    def __init__(self, env, num_iterations=300, batch_size=2000, max_ep_len=200, output_path=\"../results/\"):\n",
    "        self.output_path = output_path\n",
    "        if not os.path.exists(output_path):\n",
    "            os.makedirs(output_path)\n",
    "\n",
    "        self.env = env\n",
    "        self.observation_dim = self.env.observation_space.shape[0]\n",
    "        self.action_dim = self.env.action_space.n\n",
    "        self.gamma = 0.9\n",
    "        self.num_iterations = num_iterations\n",
    "        self.batch_size = batch_size\n",
    "        self.max_ep_len = max_ep_len\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=3e-2)\n",
    "        self.policy_net = PolicyNet(input_size=self.observation_dim, output_size=self.action_dim) # output action probability distribution\n",
    "        self.baseline_net = BaselineNet(input_size=self.observation_dim, output_size=1) # output V(s)\n",
    "\n",
    "    def play_games(self, env=None, num_episodes=None):\n",
    "        episode = 0\n",
    "        episode_rewards = []\n",
    "        paths = []\n",
    "        t = 0\n",
    "        if not env:\n",
    "            env = self.env\n",
    "\n",
    "        while (num_episodes or t < self.batch_size):\n",
    "            state = env.reset()\n",
    "            states, actions, rewards = [], [], []\n",
    "            episode_reward = 0\n",
    "\n",
    "            for step in range(self.max_ep_len):\n",
    "                states.append(state)\n",
    "                action = self.policy_net.sample_action(np.atleast_2d(state))[0] # creating batch\n",
    "                state, reward, done, _ = env.step(action)\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                episode_reward += reward\n",
    "                t += 1\n",
    "\n",
    "                if (done or step == self.max_ep_len-1):\n",
    "                    episode_rewards.append(episode_reward)\n",
    "                    break\n",
    "                if (not num_episodes) and t == self.batch_size:\n",
    "                    break\n",
    "\n",
    "            # each episode's path\n",
    "            path = {\"observation\": np.array(states),\n",
    "                    \"reward\": np.array(rewards),\n",
    "                    \"action\": np.array(actions)}\n",
    "            paths.append(path)\n",
    "            episode += 1\n",
    "            if num_episodes and episode >= num_episodes:\n",
    "                break\n",
    "\n",
    "        return paths, episode_rewards # paths is the transitions of all episodes\n",
    "\n",
    "    def get_returns(self, paths):\n",
    "        all_returns = []\n",
    "        for path in paths:\n",
    "            rewards = path[\"reward\"] # a list of rewards in an episode\n",
    "            returns = []\n",
    "            reversed_rewards = np.flip(rewards,0)\n",
    "            g_t = 0\n",
    "\n",
    "            # discounted return of an episode\n",
    "            for r in reversed_rewards:\n",
    "                g_t = r + self.gamma * g_t\n",
    "                returns.insert(0, g_t)\n",
    "\n",
    "            # discounted return of all episodes\n",
    "            all_returns.append(returns)\n",
    "\n",
    "        returns = np.concatenate(all_returns)\n",
    "        return returns\n",
    "\n",
    "    def get_advantage(self, returns, observations):\n",
    "        values = self.baseline_net.forward(observations).numpy() # calculate b\n",
    "        advantages = returns - values # G - b\n",
    "\n",
    "        # normalize\n",
    "        advantages = (advantages-np.mean(advantages)) / np.sqrt(np.sum(advantages**2))\n",
    "        return advantages\n",
    "\n",
    "    def update_policy(self, observations, actions, advantages):\n",
    "        observations = tf.convert_to_tensor(observations)\n",
    "        actions = tf.convert_to_tensor(actions)\n",
    "        advantages = tf.convert_to_tensor(advantages)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            log_prob = self.policy_net.action_distribution(observations).log_prob(actions)\n",
    "            loss = -tf.math.reduce_mean(log_prob * tf.cast(advantages, tf.float32)) # mean of batch\n",
    "\n",
    "        # backprob\n",
    "        grads = tape.gradient(loss, self.policy_net.model.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.policy_net.model.trainable_weights))\n",
    "\n",
    "    def train(self):\n",
    "        all_total_rewards = []\n",
    "        averaged_total_rewards = []\n",
    "\n",
    "        for t in range(self.num_iterations):\n",
    "            paths, total_rewards = self.play_games()\n",
    "            all_total_rewards.extend(total_rewards)\n",
    "            observations = np.concatenate([path[\"observation\"] for path in paths])\n",
    "            actions = np.concatenate([path[\"action\"] for path in paths])\n",
    "            returns = self.get_returns(paths)\n",
    "            advantages = self.get_advantage(returns, observations)\n",
    "            self.baseline_net.update(observations, returns)\n",
    "            self.update_policy(observations, actions, advantages)\n",
    "            avg_reward = np.mean(total_rewards)\n",
    "            averaged_total_rewards.append(avg_reward)\n",
    "            print(\"Average reward for batch {}: {:04.2f}\".format(t,avg_reward))\n",
    "\n",
    "        print(\"Training complete\")\n",
    "        np.save(self.output_path+ \"rewards.npy\", averaged_total_rewards)\n",
    "        export_plot(averaged_total_rewards, \"Reward\", \"CartPole-v0\", self.output_path + \"rewards.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward for batch 0: 19.96\n",
      "Average reward for batch 1: 29.38\n",
      "Average reward for batch 2: 43.37\n",
      "Average reward for batch 3: 55.74\n",
      "Average reward for batch 4: 61.16\n",
      "Average reward for batch 5: 74.81\n",
      "Average reward for batch 6: 94.50\n",
      "Average reward for batch 7: 125.87\n",
      "Average reward for batch 8: 128.13\n",
      "Average reward for batch 9: 176.82\n",
      "Average reward for batch 10: 163.17\n",
      "Average reward for batch 11: 196.40\n",
      "Average reward for batch 12: 187.20\n",
      "Average reward for batch 13: 196.50\n",
      "Average reward for batch 14: 200.00\n",
      "Average reward for batch 15: 199.20\n",
      "Average reward for batch 16: 178.64\n",
      "Average reward for batch 17: 148.85\n",
      "Average reward for batch 18: 157.42\n",
      "Average reward for batch 19: 193.00\n",
      "Average reward for batch 20: 200.00\n",
      "Average reward for batch 21: 200.00\n",
      "Average reward for batch 22: 200.00\n",
      "Average reward for batch 23: 200.00\n",
      "Average reward for batch 24: 183.30\n",
      "Average reward for batch 25: 147.23\n",
      "Average reward for batch 26: 126.13\n",
      "Average reward for batch 27: 97.10\n",
      "Average reward for batch 28: 73.59\n",
      "Average reward for batch 29: 46.26\n",
      "Average reward for batch 30: 72.92\n",
      "Average reward for batch 31: 99.60\n",
      "Average reward for batch 32: 115.18\n",
      "Average reward for batch 33: 128.67\n",
      "Average reward for batch 34: 142.08\n",
      "Average reward for batch 35: 162.58\n",
      "Average reward for batch 36: 182.60\n",
      "Average reward for batch 37: 199.10\n",
      "Average reward for batch 38: 200.00\n",
      "Average reward for batch 39: 200.00\n",
      "Average reward for batch 40: 200.00\n",
      "Average reward for batch 41: 200.00\n",
      "Average reward for batch 42: 200.00\n",
      "Average reward for batch 43: 200.00\n",
      "Average reward for batch 44: 200.00\n",
      "Average reward for batch 45: 200.00\n",
      "Average reward for batch 46: 200.00\n",
      "Average reward for batch 47: 200.00\n",
      "Average reward for batch 48: 200.00\n",
      "Average reward for batch 49: 200.00\n",
      "Average reward for batch 50: 200.00\n",
      "Average reward for batch 51: 200.00\n",
      "Average reward for batch 52: 200.00\n",
      "Average reward for batch 53: 200.00\n",
      "Average reward for batch 54: 200.00\n",
      "Average reward for batch 55: 200.00\n",
      "Average reward for batch 56: 200.00\n",
      "Average reward for batch 57: 200.00\n",
      "Average reward for batch 58: 200.00\n",
      "Average reward for batch 59: 200.00\n",
      "Average reward for batch 60: 200.00\n",
      "Average reward for batch 61: 200.00\n",
      "Average reward for batch 62: 200.00\n",
      "Average reward for batch 63: 200.00\n",
      "Average reward for batch 64: 200.00\n",
      "Average reward for batch 65: 200.00\n",
      "Average reward for batch 66: 200.00\n",
      "Average reward for batch 67: 200.00\n",
      "Average reward for batch 68: 153.33\n",
      "Average reward for batch 69: 200.00\n",
      "Average reward for batch 70: 200.00\n",
      "Average reward for batch 71: 200.00\n",
      "Average reward for batch 72: 200.00\n",
      "Average reward for batch 73: 200.00\n",
      "Average reward for batch 74: 200.00\n",
      "Average reward for batch 75: 200.00\n",
      "Average reward for batch 76: 200.00\n",
      "Average reward for batch 77: 200.00\n",
      "Average reward for batch 78: 200.00\n",
      "Average reward for batch 79: 200.00\n",
      "Average reward for batch 80: 200.00\n",
      "Average reward for batch 81: 200.00\n",
      "Average reward for batch 82: 200.00\n",
      "Average reward for batch 83: 200.00\n",
      "Average reward for batch 84: 200.00\n",
      "Average reward for batch 85: 200.00\n",
      "Average reward for batch 86: 200.00\n",
      "Average reward for batch 87: 200.00\n",
      "Average reward for batch 88: 200.00\n",
      "Average reward for batch 89: 200.00\n",
      "Average reward for batch 90: 200.00\n",
      "Average reward for batch 91: 200.00\n",
      "Average reward for batch 92: 200.00\n",
      "Average reward for batch 93: 200.00\n",
      "Average reward for batch 94: 200.00\n",
      "Average reward for batch 95: 200.00\n",
      "Average reward for batch 96: 200.00\n",
      "Average reward for batch 97: 200.00\n",
      "Average reward for batch 98: 200.00\n",
      "Average reward for batch 99: 200.00\n",
      "Average reward for batch 100: 200.00\n",
      "Average reward for batch 101: 198.10\n",
      "Average reward for batch 102: 171.45\n",
      "Average reward for batch 103: 142.92\n",
      "Average reward for batch 104: 120.00\n",
      "Average reward for batch 105: 113.88\n",
      "Average reward for batch 106: 105.50\n",
      "Average reward for batch 107: 102.47\n",
      "Average reward for batch 108: 104.05\n",
      "Average reward for batch 109: 100.63\n",
      "Average reward for batch 110: 105.83\n",
      "Average reward for batch 111: 109.83\n",
      "Average reward for batch 112: 112.88\n",
      "Average reward for batch 113: 114.59\n",
      "Average reward for batch 114: 117.06\n",
      "Average reward for batch 115: 117.00\n",
      "Average reward for batch 116: 120.50\n",
      "Average reward for batch 117: 122.56\n",
      "Average reward for batch 118: 127.73\n",
      "Average reward for batch 119: 136.79\n",
      "Average reward for batch 120: 132.47\n",
      "Average reward for batch 121: 139.93\n",
      "Average reward for batch 122: 148.54\n",
      "Average reward for batch 123: 158.42\n",
      "Average reward for batch 124: 178.36\n",
      "Average reward for batch 125: 193.60\n",
      "Average reward for batch 126: 200.00\n",
      "Average reward for batch 127: 200.00\n",
      "Average reward for batch 128: 200.00\n",
      "Average reward for batch 129: 200.00\n",
      "Average reward for batch 130: 200.00\n",
      "Average reward for batch 131: 200.00\n",
      "Average reward for batch 132: 200.00\n",
      "Average reward for batch 133: 200.00\n",
      "Average reward for batch 134: 200.00\n",
      "Average reward for batch 135: 200.00\n",
      "Average reward for batch 136: 200.00\n",
      "Average reward for batch 137: 200.00\n",
      "Average reward for batch 138: 200.00\n",
      "Average reward for batch 139: 200.00\n",
      "Average reward for batch 140: 200.00\n",
      "Average reward for batch 141: 200.00\n",
      "Average reward for batch 142: 200.00\n",
      "Average reward for batch 143: 200.00\n",
      "Average reward for batch 144: 200.00\n",
      "Average reward for batch 145: 200.00\n",
      "Average reward for batch 146: 200.00\n",
      "Average reward for batch 147: 200.00\n",
      "Average reward for batch 148: 200.00\n",
      "Average reward for batch 149: 200.00\n",
      "Average reward for batch 150: 200.00\n",
      "Average reward for batch 151: 199.80\n",
      "Average reward for batch 152: 196.70\n",
      "Average reward for batch 153: 185.20\n",
      "Average reward for batch 154: 163.83\n",
      "Average reward for batch 155: 146.15\n",
      "Average reward for batch 156: 137.86\n",
      "Average reward for batch 157: 121.81\n",
      "Average reward for batch 158: 116.18\n",
      "Average reward for batch 159: 112.82\n",
      "Average reward for batch 160: 108.06\n",
      "Average reward for batch 161: 107.83\n",
      "Average reward for batch 162: 107.56\n",
      "Average reward for batch 163: 106.17\n",
      "Average reward for batch 164: 102.74\n",
      "Average reward for batch 165: 100.37\n",
      "Average reward for batch 166: 70.11\n",
      "Average reward for batch 167: 76.46\n",
      "Average reward for batch 168: 99.60\n",
      "Average reward for batch 169: 103.42\n",
      "Average reward for batch 170: 103.00\n",
      "Average reward for batch 171: 107.28\n",
      "Average reward for batch 172: 108.22\n",
      "Average reward for batch 173: 109.00\n",
      "Average reward for batch 174: 108.78\n",
      "Average reward for batch 175: 114.06\n",
      "Average reward for batch 176: 119.44\n",
      "Average reward for batch 177: 117.65\n",
      "Average reward for batch 178: 116.65\n",
      "Average reward for batch 179: 120.12\n",
      "Average reward for batch 180: 120.69\n",
      "Average reward for batch 181: 123.62\n",
      "Average reward for batch 182: 126.73\n",
      "Average reward for batch 183: 125.27\n",
      "Average reward for batch 184: 127.40\n",
      "Average reward for batch 185: 125.93\n",
      "Average reward for batch 186: 125.27\n",
      "Average reward for batch 187: 119.38\n",
      "Average reward for batch 188: 124.19\n",
      "Average reward for batch 189: 119.62\n",
      "Average reward for batch 190: 118.94\n",
      "Average reward for batch 191: 112.94\n",
      "Average reward for batch 192: 116.29\n",
      "Average reward for batch 193: 112.53\n",
      "Average reward for batch 194: 114.35\n",
      "Average reward for batch 195: 114.00\n",
      "Average reward for batch 196: 114.29\n",
      "Average reward for batch 197: 115.47\n",
      "Average reward for batch 198: 114.65\n",
      "Average reward for batch 199: 119.75\n",
      "Average reward for batch 200: 121.06\n",
      "Average reward for batch 201: 120.25\n",
      "Average reward for batch 202: 118.25\n",
      "Average reward for batch 203: 124.12\n",
      "Average reward for batch 204: 118.00\n",
      "Average reward for batch 205: 118.94\n",
      "Average reward for batch 206: 120.44\n",
      "Average reward for batch 207: 115.82\n",
      "Average reward for batch 208: 117.12\n",
      "Average reward for batch 209: 114.59\n",
      "Average reward for batch 210: 115.00\n",
      "Average reward for batch 211: 113.59\n",
      "Average reward for batch 212: 112.47\n",
      "Average reward for batch 213: 112.94\n",
      "Average reward for batch 214: 114.18\n",
      "Average reward for batch 215: 113.94\n",
      "Average reward for batch 216: 114.47\n",
      "Average reward for batch 217: 116.94\n",
      "Average reward for batch 218: 118.50\n",
      "Average reward for batch 219: 127.00\n",
      "Average reward for batch 220: 125.60\n",
      "Average reward for batch 221: 130.00\n",
      "Average reward for batch 222: 128.80\n",
      "Average reward for batch 223: 129.53\n",
      "Average reward for batch 224: 127.20\n",
      "Average reward for batch 225: 129.60\n",
      "Average reward for batch 226: 128.07\n",
      "Average reward for batch 227: 123.06\n",
      "Average reward for batch 228: 122.00\n",
      "Average reward for batch 229: 119.12\n",
      "Average reward for batch 230: 116.18\n",
      "Average reward for batch 231: 119.06\n",
      "Average reward for batch 232: 116.35\n",
      "Average reward for batch 233: 115.59\n",
      "Average reward for batch 234: 114.29\n",
      "Average reward for batch 235: 114.41\n",
      "Average reward for batch 236: 115.88\n",
      "Average reward for batch 237: 119.00\n",
      "Average reward for batch 238: 120.56\n",
      "Average reward for batch 239: 120.94\n",
      "Average reward for batch 240: 122.69\n",
      "Average reward for batch 241: 122.12\n",
      "Average reward for batch 242: 123.25\n",
      "Average reward for batch 243: 123.62\n",
      "Average reward for batch 244: 120.69\n",
      "Average reward for batch 245: 123.69\n",
      "Average reward for batch 246: 125.67\n",
      "Average reward for batch 247: 128.13\n",
      "Average reward for batch 248: 128.13\n",
      "Average reward for batch 249: 136.36\n",
      "Average reward for batch 250: 140.71\n",
      "Average reward for batch 251: 137.86\n",
      "Average reward for batch 252: 145.31\n",
      "Average reward for batch 253: 142.57\n",
      "Average reward for batch 254: 144.31\n",
      "Average reward for batch 255: 151.15\n",
      "Average reward for batch 256: 142.21\n",
      "Average reward for batch 257: 141.50\n",
      "Average reward for batch 258: 144.62\n",
      "Average reward for batch 259: 142.69\n",
      "Average reward for batch 260: 136.43\n",
      "Average reward for batch 261: 128.60\n",
      "Average reward for batch 262: 137.21\n",
      "Average reward for batch 263: 130.93\n",
      "Average reward for batch 264: 132.87\n",
      "Average reward for batch 265: 129.60\n",
      "Average reward for batch 266: 131.93\n",
      "Average reward for batch 267: 124.50\n",
      "Average reward for batch 268: 126.60\n",
      "Average reward for batch 269: 121.75\n",
      "Average reward for batch 270: 114.00\n",
      "Average reward for batch 271: 115.24\n",
      "Average reward for batch 272: 117.62\n",
      "Average reward for batch 273: 114.59\n",
      "Average reward for batch 274: 118.25\n",
      "Average reward for batch 275: 115.18\n",
      "Average reward for batch 276: 119.69\n",
      "Average reward for batch 277: 122.06\n",
      "Average reward for batch 278: 127.47\n",
      "Average reward for batch 279: 133.27\n",
      "Average reward for batch 280: 139.21\n",
      "Average reward for batch 281: 143.38\n",
      "Average reward for batch 282: 147.38\n",
      "Average reward for batch 283: 156.08\n",
      "Average reward for batch 284: 162.75\n",
      "Average reward for batch 285: 169.64\n",
      "Average reward for batch 286: 172.45\n",
      "Average reward for batch 287: 173.64\n",
      "Average reward for batch 288: 178.82\n",
      "Average reward for batch 289: 174.82\n",
      "Average reward for batch 290: 177.73\n",
      "Average reward for batch 291: 179.64\n",
      "Average reward for batch 292: 173.91\n",
      "Average reward for batch 293: 171.55\n",
      "Average reward for batch 294: 169.55\n",
      "Average reward for batch 295: 173.18\n",
      "Average reward for batch 296: 172.82\n",
      "Average reward for batch 297: 167.82\n",
      "Average reward for batch 298: 161.25\n",
      "Average reward for batch 299: 152.85\n",
      "Training complete\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "model = PolicyGradient(env)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# old agent play game\n",
    "state = env.reset()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action = model.policy_net.sample_action(np.atleast_2d(state))[0]\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    env.render()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: D:/RL/policy-gradient/results/model\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "# save old agent\n",
    "model.policy_net.save_policy_net()\n",
    "# model.baseline_net.save_baseline_net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "# new agent\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "new_policy_net = PolicyNet(input_size=env.observation_space.shape[0], output_size=env.action_space.n)\n",
    "new_policy_net.model = keras.models.load_model(\"D:/RL/policy-gradient/results/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new agent play game\n",
    "state = env.reset()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action = new_policy_net.sample_action(np.atleast_2d(state))[0]\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    env.render()\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
