{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://aleksandarhaber.com/deep-q-networks-dqn-in-python-from-scratch-by-using-openai-gym-and-tensorflow-reinforcement-learning-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install gymnasium\n",
    "%pip install gym==0.26.2 \n",
    "%pip install tensorflow\n",
    "%pip install moviepy\n",
    "%pip install pygame --pre\n",
    "%pip install ffmpeg --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.26.2\n",
      "2.12.0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import tensorflow\n",
    "print(gym.__version__)\n",
    "print(tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary libraries\n",
    "import numpy as np\n",
    "import random\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from collections import deque \n",
    "from tensorflow import gather_nd\n",
    "from tensorflow.keras.losses import mean_squared_error \n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQLearning:\n",
    "     \n",
    "    ###########################################################################\n",
    "    #   START - __init__ function\n",
    "    ###########################################################################\n",
    "    # INPUTS: \n",
    "    # env - Cart Pole environment\n",
    "    # gamma - discount rate\n",
    "    # epsilon - parameter for epsilon-greedy approach\n",
    "    # numberEpisodes - total number of simulation episodes\n",
    "     \n",
    "             \n",
    "    def __init__(self,env,gamma,epsilon,numberEpisodes):  \n",
    "        self.env=env\n",
    "        self.gamma=gamma\n",
    "        self.epsilon=epsilon\n",
    "        self.numberEpisodes=numberEpisodes\n",
    "         \n",
    "        # state dimension\n",
    "        self.stateDimension=4\n",
    "        # action dimension\n",
    "        self.actionDimension=2\n",
    "        # this is the maximum size of the replay buffer\n",
    "        self.replayBufferSize=300\n",
    "        # this is the size of the training batch that is randomly sampled from the replay buffer\n",
    "        self.batchReplayBufferSize=100\n",
    "         \n",
    "        # number of training episodes it takes to update the target network parameters\n",
    "        # that is, every updateTargetNetworkPeriod we update the target network parameters\n",
    "        self.updateTargetNetworkPeriod=100\n",
    "         \n",
    "        # this is the counter for updating the target network \n",
    "        # if this counter exceeds (updateTargetNetworkPeriod-1) we update the network \n",
    "        # parameters and reset the counter to zero, this process is repeated until the end of the training process\n",
    "        self.counterUpdateTargetNetwork=0\n",
    "         \n",
    "        # this sum is used to store the sum of rewards obtained during each training episode\n",
    "        self.sumRewardsEpisode=[]\n",
    "         \n",
    "        # replay buffer\n",
    "        self.replayBuffer=deque(maxlen=self.replayBufferSize)\n",
    "         \n",
    "        # this is the main network\n",
    "        # create network\n",
    "        self.mainNetwork=self.createNetwork()\n",
    "         \n",
    "        # this is the target network\n",
    "        # create network\n",
    "        self.targetNetwork=self.createNetwork()\n",
    "         \n",
    "        # copy the initial weights to targetNetwork\n",
    "        self.targetNetwork.set_weights(self.mainNetwork.get_weights())\n",
    "         \n",
    "        # this list is used in the cost function to select certain entries of the \n",
    "        # predicted and true sample matrices in order to form the loss\n",
    "        self.actionsAppend=[]\n",
    "\n",
    "    # create a neural network\n",
    "    def createNetwork(self):\n",
    "        model=Sequential()\n",
    "        model.add(Dense(128,input_dim=self.stateDimension,activation='relu'))\n",
    "        model.add(Dense(56,activation='relu'))\n",
    "        model.add(Dense(self.actionDimension,activation='linear'))\n",
    "        # compile the network with the custom loss defined in my_loss_fn\n",
    "        model.compile(optimizer = RMSprop(), loss = self.my_loss_fn, metrics = ['accuracy'])\n",
    "        return model\n",
    "    \n",
    "    def my_loss_fn(self,y_true, y_pred):\n",
    "        \n",
    "       s1,s2=y_true.shape\n",
    "       #print(s1,s2)\n",
    "        \n",
    "       # this matrix defines indices of a set of entries that we want to \n",
    "       # extract from y_true and y_pred\n",
    "       # s2=2\n",
    "       # s1=self.batchReplayBufferSize\n",
    "       indices=np.zeros(shape=(s1,s2))\n",
    "       indices[:,0]=np.arange(s1)\n",
    "       indices[:,1]=self.actionsAppend\n",
    "        \n",
    "       # gather_nd and mean_squared_error are TensorFlow functions\n",
    "       loss = mean_squared_error(gather_nd(y_true,indices=indices.astype(int)), gather_nd(y_pred,indices=indices.astype(int)))\n",
    "       #print(loss)\n",
    "       return loss\n",
    "    \n",
    "    def trainingEpisodes(self):\n",
    "    \n",
    "         \n",
    "        # here we loop through the episodes\n",
    "        for indexEpisode in range(self.numberEpisodes):\n",
    "             \n",
    "            # list that stores rewards per episode - this is necessary for keeping track of convergence \n",
    "            rewardsEpisode=[]\n",
    "                        \n",
    "            print(\"Simulating episode {}\".format(indexEpisode))\n",
    "             \n",
    "            # reset the environment at the beginning of every episode\n",
    "            (currentState,_)=self.env.reset()\n",
    "                       \n",
    "            # here we step from one state to another\n",
    "            # this will loop until a terminal state is reached\n",
    "            terminalState=False\n",
    "            while not terminalState:\n",
    "                                       \n",
    "                # select an action on the basis of the current state, denoted by currentState\n",
    "                action = self.selectAction(currentState,indexEpisode)\n",
    "                 \n",
    "                # here we step and return the state, reward, and boolean denoting if the state is a terminal state\n",
    "                (nextState, reward, terminalState,_,_) = self.env.step(action)          \n",
    "                rewardsEpisode.append(reward)\n",
    "          \n",
    "                # add current state, action, reward, next state, and terminal flag to the replay buffer\n",
    "                self.replayBuffer.append((currentState,action,reward,nextState,terminalState))\n",
    "                 \n",
    "                # train network\n",
    "                self.trainNetwork()\n",
    "                 \n",
    "                # set the current state for the next step\n",
    "                currentState=nextState\n",
    "             \n",
    "            print(\"Sum of rewards {}\".format(np.sum(rewardsEpisode)))        \n",
    "            self.sumRewardsEpisode.append(np.sum(rewardsEpisode))\n",
    "\n",
    "    def selectAction(self,state,index):\n",
    "       import numpy as np\n",
    "        \n",
    "       # first index episodes we select completely random actions to have enough exploration\n",
    "       # change this\n",
    "       if index<1:\n",
    "           return np.random.choice(self.actionDimension)   \n",
    "            \n",
    "       # Returns a random real number in the half-open interval [0.0, 1.0)\n",
    "       # this number is used for the epsilon greedy approach\n",
    "       randomNumber=np.random.random()\n",
    "        \n",
    "       # after index episodes, we slowly start to decrease the epsilon parameter\n",
    "       if index>200:\n",
    "           self.epsilon=0.999*self.epsilon\n",
    "        \n",
    "       # if this condition is satisfied, we are exploring, that is, we select random actions\n",
    "       if randomNumber < self.epsilon:\n",
    "           # returns a random action selected from: 0,1,...,actionNumber-1\n",
    "           return np.random.choice(self.actionDimension)            \n",
    "        \n",
    "       # otherwise, we are selecting greedy actions\n",
    "       else:\n",
    "           # we return the index where Qvalues[state,:] has the max value\n",
    "           # that is, since the index denotes an action, we select greedy actions\n",
    "                       \n",
    "           Qvalues=self.mainNetwork.predict(state.reshape(1,4), verbose=0)\n",
    "          \n",
    "           return np.random.choice(np.where(Qvalues[0,:]==np.max(Qvalues[0,:]))[0])\n",
    "           # here we need to return the minimum index since it can happen\n",
    "           # that there are several identical maximal entries, for example \n",
    "           # import numpy as np\n",
    "           # a=[0,1,1,0]\n",
    "           # np.where(a==np.max(a))\n",
    "           # this will return [1,2], but we only need a single index\n",
    "           # that is why we need to have np.random.choice(np.where(a==np.max(a))[0])\n",
    "           # note that zero has to be added here since np.where() returns a tuple\n",
    "\n",
    "    def trainNetwork(self):\n",
    " \n",
    "        # if the replay buffer has at least batchReplayBufferSize elements,\n",
    "        # then train the model \n",
    "        # otherwise wait until the size of the elements exceeds batchReplayBufferSize\n",
    "        if (len(self.replayBuffer)>self.batchReplayBufferSize):\n",
    "             \n",
    " \n",
    "            # sample a batch from the replay buffer\n",
    "            randomSampleBatch=random.sample(self.replayBuffer, self.batchReplayBufferSize)\n",
    "             \n",
    "            # here we form current state batch \n",
    "            # and next state batch\n",
    "            # they are used as inputs for prediction\n",
    "            currentStateBatch=np.zeros(shape=(self.batchReplayBufferSize,4))\n",
    "            nextStateBatch=np.zeros(shape=(self.batchReplayBufferSize,4))            \n",
    "            # this will enumerate the tuple entries of the randomSampleBatch\n",
    "            # index will loop through the number of tuples\n",
    "            for index,tupleS in enumerate(randomSampleBatch):\n",
    "                # first entry of the tuple is the current state\n",
    "                currentStateBatch[index,:]=tupleS[0]\n",
    "                # fourth entry of the tuple is the next state\n",
    "                nextStateBatch[index,:]=tupleS[3]\n",
    "             \n",
    "            # here, use the target network to predict Q-values \n",
    "            QnextStateTargetNetwork=self.targetNetwork.predict(nextStateBatch, verbose=0)\n",
    "            # here, use the main network to predict Q-values \n",
    "            QcurrentStateMainNetwork=self.mainNetwork.predict(currentStateBatch, verbose=0)\n",
    "             \n",
    "            # now, we form batches for training\n",
    "            # input for training\n",
    "            inputNetwork=currentStateBatch\n",
    "            # output for training\n",
    "            outputNetwork=np.zeros(shape=(self.batchReplayBufferSize,2))\n",
    "             \n",
    "            # this list will contain the actions that are selected from the batch \n",
    "            # this list is used in my_loss_fn to define the loss-function\n",
    "            self.actionsAppend=[]            \n",
    "            for index,(currentState,action,reward,nextState,terminated) in enumerate(randomSampleBatch):\n",
    "                 \n",
    "                # if the next state is the terminal state\n",
    "                if terminated:\n",
    "                    y=reward                  \n",
    "                # if the next state if not the terminal state    \n",
    "                else:\n",
    "                    y=reward+self.gamma*np.max(QnextStateTargetNetwork[index])\n",
    "                 \n",
    "                # this is necessary for defining the cost function\n",
    "                self.actionsAppend.append(action)\n",
    "                 \n",
    "                # this actually does not matter since we do not use all the entries in the cost function\n",
    "                outputNetwork[index]=QcurrentStateMainNetwork[index]\n",
    "                # this is what matters\n",
    "                outputNetwork[index,action]=y\n",
    "             \n",
    "            # here, we train the network\n",
    "            self.mainNetwork.fit(inputNetwork,outputNetwork,batch_size = self.batchReplayBufferSize, verbose=0, epochs=100)     \n",
    "             \n",
    "            # after updateTargetNetworkPeriod training sessions, update the coefficients \n",
    "            # of the target network\n",
    "            # increase the counter for training the target network\n",
    "            self.counterUpdateTargetNetwork+=1 \n",
    "            if (self.counterUpdateTargetNetwork>(self.updateTargetNetworkPeriod-1)):\n",
    "                # copy the weights to targetNetwork\n",
    "                self.targetNetwork.set_weights(self.mainNetwork.get_weights())        \n",
    "                print(\"Target network updated!\")\n",
    "                print(\"Counter value {}\".format(self.counterUpdateTargetNetwork))\n",
    "                # reset the counter\n",
    "                self.counterUpdateTargetNetwork=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------------------------------\n",
    "# training (can skip)\n",
    "#------------------------------------------------------------------------------------------------------\n",
    "\n",
    "env=gym.make('CartPole-v1')\n",
    "# select the parameters\n",
    "gamma=1\n",
    "# probability parameter for the epsilon-greedy approach\n",
    "epsilon=0.1\n",
    "numberEpisodes=1000\n",
    "\n",
    "# create an object\n",
    "LearningQDeep=DeepQLearning(env,gamma,epsilon,numberEpisodes)\n",
    "# run the learning process\n",
    "LearningQDeep.trainingEpisodes()\n",
    "# get the obtained rewards in every episode\n",
    "LearningQDeep.sumRewardsEpisode\n",
    "\n",
    "#  summarize the model\n",
    "LearningQDeep.mainNetwork.summary()\n",
    "# save the model, this is important, since it takes long time to train the model \n",
    "# and we will need model in another file to visualize the trained model performance\n",
    "LearningQDeep.mainNetwork.save(\"trained_model_temp.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\RL\\gym_version_new\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\monitoring\\video_recorder.py:182: UserWarning: \u001b[33mWARN: Unable to save last video! Did you call close()?\u001b[0m\n",
      "  logger.warn(\"Unable to save last video! Did you call close()?\")\n",
      "d:\\RL\\gym_version_new\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\record_video.py:87: UserWarning: \u001b[33mWARN: Overwriting existing videos at d:\\RL\\gym_version_new\\stored_video folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video d:\\RL\\gym_version_new\\stored_video\\rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video d:\\RL\\gym_version_new\\stored_video\\rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready d:\\RL\\gym_version_new\\stored_video\\rl-video-episode-0.mp4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 33\u001b[0m\n\u001b[0;32m     30\u001b[0m terminalState\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m terminalState:\n\u001b[0;32m     32\u001b[0m     \u001b[39m# get the Q-value (1 by 2 vector)\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m     Qvalues\u001b[39m=\u001b[39mloaded_model\u001b[39m.\u001b[39;49mpredict(currentState\u001b[39m.\u001b[39;49mreshape(\u001b[39m1\u001b[39;49m,\u001b[39m4\u001b[39;49m), verbose\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[0;32m     34\u001b[0m     \u001b[39m# select the action that gives the max Qvalue\u001b[39;00m\n\u001b[0;32m     35\u001b[0m     action\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mchoice(np\u001b[39m.\u001b[39mwhere(Qvalues[\u001b[39m0\u001b[39m,:]\u001b[39m==\u001b[39mnp\u001b[39m.\u001b[39mmax(Qvalues[\u001b[39m0\u001b[39m,:]))[\u001b[39m0\u001b[39m])\n",
      "File \u001b[1;32md:\\RL\\gym_version_new\\.venv\\Lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32md:\\RL\\gym_version_new\\.venv\\Lib\\site-packages\\keras\\engine\\training.py:2349\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   2340\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[0;32m   2341\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   2342\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mUsing Model.predict with MultiWorkerMirroredStrategy \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2343\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mor TPUStrategy and AutoShardPolicy.FILE might lead to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2346\u001b[0m             stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[0;32m   2347\u001b[0m         )\n\u001b[1;32m-> 2349\u001b[0m data_handler \u001b[39m=\u001b[39m data_adapter\u001b[39m.\u001b[39;49mget_data_handler(\n\u001b[0;32m   2350\u001b[0m     x\u001b[39m=\u001b[39;49mx,\n\u001b[0;32m   2351\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m   2352\u001b[0m     steps_per_epoch\u001b[39m=\u001b[39;49msteps,\n\u001b[0;32m   2353\u001b[0m     initial_epoch\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[0;32m   2354\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[0;32m   2355\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[0;32m   2356\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[0;32m   2357\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[0;32m   2358\u001b[0m     model\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m   2359\u001b[0m     steps_per_execution\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_steps_per_execution,\n\u001b[0;32m   2360\u001b[0m )\n\u001b[0;32m   2362\u001b[0m \u001b[39m# Container that configures and calls `tf.keras.Callback`s.\u001b[39;00m\n\u001b[0;32m   2363\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(callbacks, callbacks_module\u001b[39m.\u001b[39mCallbackList):\n",
      "File \u001b[1;32md:\\RL\\gym_version_new\\.venv\\Lib\\site-packages\\keras\\engine\\data_adapter.py:1583\u001b[0m, in \u001b[0;36mget_data_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1581\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(kwargs[\u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39m_cluster_coordinator\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m   1582\u001b[0m     \u001b[39mreturn\u001b[39;00m _ClusterCoordinatorDataHandler(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m-> 1583\u001b[0m \u001b[39mreturn\u001b[39;00m DataHandler(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32md:\\RL\\gym_version_new\\.venv\\Lib\\site-packages\\keras\\engine\\data_adapter.py:1260\u001b[0m, in \u001b[0;36mDataHandler.__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[0;32m   1257\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_steps_per_execution \u001b[39m=\u001b[39m steps_per_execution\n\u001b[0;32m   1259\u001b[0m adapter_cls \u001b[39m=\u001b[39m select_data_adapter(x, y)\n\u001b[1;32m-> 1260\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_adapter \u001b[39m=\u001b[39m adapter_cls(\n\u001b[0;32m   1261\u001b[0m     x,\n\u001b[0;32m   1262\u001b[0m     y,\n\u001b[0;32m   1263\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m   1264\u001b[0m     steps\u001b[39m=\u001b[39;49msteps_per_epoch,\n\u001b[0;32m   1265\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs \u001b[39m-\u001b[39;49m initial_epoch,\n\u001b[0;32m   1266\u001b[0m     sample_weights\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m   1267\u001b[0m     shuffle\u001b[39m=\u001b[39;49mshuffle,\n\u001b[0;32m   1268\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[0;32m   1269\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[0;32m   1270\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[0;32m   1271\u001b[0m     distribution_strategy\u001b[39m=\u001b[39;49mtf\u001b[39m.\u001b[39;49mdistribute\u001b[39m.\u001b[39;49mget_strategy(),\n\u001b[0;32m   1272\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[0;32m   1273\u001b[0m )\n\u001b[0;32m   1275\u001b[0m strategy \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mdistribute\u001b[39m.\u001b[39mget_strategy()\n\u001b[0;32m   1277\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_current_step \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[1;32md:\\RL\\gym_version_new\\.venv\\Lib\\site-packages\\keras\\engine\\data_adapter.py:348\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.__init__\u001b[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[0;32m    344\u001b[0m     \u001b[39mreturn\u001b[39;00m flat_dataset\n\u001b[0;32m    346\u001b[0m indices_dataset \u001b[39m=\u001b[39m indices_dataset\u001b[39m.\u001b[39mflat_map(slice_batch_indices)\n\u001b[1;32m--> 348\u001b[0m dataset \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mslice_inputs(indices_dataset, inputs)\n\u001b[0;32m    350\u001b[0m \u001b[39mif\u001b[39;00m shuffle \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbatch\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    352\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mshuffle_batch\u001b[39m(\u001b[39m*\u001b[39mbatch):\n",
      "File \u001b[1;32md:\\RL\\gym_version_new\\.venv\\Lib\\site-packages\\keras\\engine\\data_adapter.py:389\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.slice_inputs\u001b[1;34m(self, indices_dataset, inputs)\u001b[0m\n\u001b[0;32m    384\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgrab_batch\u001b[39m(i, data):\n\u001b[0;32m    385\u001b[0m     \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mmap_structure(\n\u001b[0;32m    386\u001b[0m         \u001b[39mlambda\u001b[39;00m d: tf\u001b[39m.\u001b[39mgather(d, i, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m), data\n\u001b[0;32m    387\u001b[0m     )\n\u001b[1;32m--> 389\u001b[0m dataset \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39;49mmap(grab_batch, num_parallel_calls\u001b[39m=\u001b[39;49mtf\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mAUTOTUNE)\n\u001b[0;32m    391\u001b[0m \u001b[39m# Default optimizations are disabled to avoid the overhead of\u001b[39;00m\n\u001b[0;32m    392\u001b[0m \u001b[39m# (unnecessary) input pipeline graph serialization and deserialization\u001b[39;00m\n\u001b[0;32m    393\u001b[0m options \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mOptions()\n",
      "File \u001b[1;32md:\\RL\\gym_version_new\\.venv\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:2240\u001b[0m, in \u001b[0;36mDatasetV2.map\u001b[1;34m(self, map_func, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[0;32m   2236\u001b[0m \u001b[39m# Loaded lazily due to a circular dependency (dataset_ops -> map_op ->\u001b[39;00m\n\u001b[0;32m   2237\u001b[0m \u001b[39m# dataset_ops).\u001b[39;00m\n\u001b[0;32m   2238\u001b[0m \u001b[39m# pylint: disable=g-import-not-at-top,protected-access\u001b[39;00m\n\u001b[0;32m   2239\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m \u001b[39mimport\u001b[39;00m map_op\n\u001b[1;32m-> 2240\u001b[0m \u001b[39mreturn\u001b[39;00m map_op\u001b[39m.\u001b[39;49m_map_v2(\n\u001b[0;32m   2241\u001b[0m     \u001b[39mself\u001b[39;49m,\n\u001b[0;32m   2242\u001b[0m     map_func,\n\u001b[0;32m   2243\u001b[0m     num_parallel_calls\u001b[39m=\u001b[39;49mnum_parallel_calls,\n\u001b[0;32m   2244\u001b[0m     deterministic\u001b[39m=\u001b[39;49mdeterministic,\n\u001b[0;32m   2245\u001b[0m     name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[1;32md:\\RL\\gym_version_new\\.venv\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\map_op.py:40\u001b[0m, in \u001b[0;36m_map_v2\u001b[1;34m(input_dataset, map_func, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[0;32m     37\u001b[0m   \u001b[39mreturn\u001b[39;00m _MapDataset(\n\u001b[0;32m     38\u001b[0m       input_dataset, map_func, preserve_cardinality\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, name\u001b[39m=\u001b[39mname)\n\u001b[0;32m     39\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 40\u001b[0m   \u001b[39mreturn\u001b[39;00m _ParallelMapDataset(\n\u001b[0;32m     41\u001b[0m       input_dataset,\n\u001b[0;32m     42\u001b[0m       map_func,\n\u001b[0;32m     43\u001b[0m       num_parallel_calls\u001b[39m=\u001b[39;49mnum_parallel_calls,\n\u001b[0;32m     44\u001b[0m       deterministic\u001b[39m=\u001b[39;49mdeterministic,\n\u001b[0;32m     45\u001b[0m       preserve_cardinality\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m     46\u001b[0m       name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[1;32md:\\RL\\gym_version_new\\.venv\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\map_op.py:163\u001b[0m, in \u001b[0;36m_ParallelMapDataset.__init__\u001b[1;34m(self, input_dataset, map_func, num_parallel_calls, deterministic, use_inter_op_parallelism, preserve_cardinality, use_legacy_function, name)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_parallel_calls \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39mconvert_to_tensor(\n\u001b[0;32m    161\u001b[0m     num_parallel_calls, dtype\u001b[39m=\u001b[39mdtypes\u001b[39m.\u001b[39mint64, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mnum_parallel_calls\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    162\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_name \u001b[39m=\u001b[39m name\n\u001b[1;32m--> 163\u001b[0m variant_tensor \u001b[39m=\u001b[39m gen_dataset_ops\u001b[39m.\u001b[39;49mparallel_map_dataset_v2(\n\u001b[0;32m    164\u001b[0m     input_dataset\u001b[39m.\u001b[39;49m_variant_tensor,  \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    165\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_map_func\u001b[39m.\u001b[39;49mfunction\u001b[39m.\u001b[39;49mcaptured_inputs,\n\u001b[0;32m    166\u001b[0m     f\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_map_func\u001b[39m.\u001b[39;49mfunction,\n\u001b[0;32m    167\u001b[0m     num_parallel_calls\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_parallel_calls,\n\u001b[0;32m    168\u001b[0m     deterministic\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_deterministic,\n\u001b[0;32m    169\u001b[0m     use_inter_op_parallelism\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_use_inter_op_parallelism,\n\u001b[0;32m    170\u001b[0m     preserve_cardinality\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_preserve_cardinality,\n\u001b[0;32m    171\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_common_args)\n\u001b[0;32m    172\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(input_dataset, variant_tensor)\n",
      "File \u001b[1;32md:\\RL\\gym_version_new\\.venv\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py:5945\u001b[0m, in \u001b[0;36mparallel_map_dataset_v2\u001b[1;34m(input_dataset, other_arguments, num_parallel_calls, f, output_types, output_shapes, use_inter_op_parallelism, deterministic, preserve_cardinality, metadata, name)\u001b[0m\n\u001b[0;32m   5943\u001b[0m \u001b[39mif\u001b[39;00m tld\u001b[39m.\u001b[39mis_eager:\n\u001b[0;32m   5944\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 5945\u001b[0m     _result \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_FastPathExecute(\n\u001b[0;32m   5946\u001b[0m       _ctx, \u001b[39m\"\u001b[39;49m\u001b[39mParallelMapDatasetV2\u001b[39;49m\u001b[39m\"\u001b[39;49m, name, input_dataset, other_arguments,\n\u001b[0;32m   5947\u001b[0m       num_parallel_calls, \u001b[39m\"\u001b[39;49m\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m, f, \u001b[39m\"\u001b[39;49m\u001b[39moutput_types\u001b[39;49m\u001b[39m\"\u001b[39;49m, output_types,\n\u001b[0;32m   5948\u001b[0m       \u001b[39m\"\u001b[39;49m\u001b[39moutput_shapes\u001b[39;49m\u001b[39m\"\u001b[39;49m, output_shapes, \u001b[39m\"\u001b[39;49m\u001b[39muse_inter_op_parallelism\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   5949\u001b[0m       use_inter_op_parallelism, \u001b[39m\"\u001b[39;49m\u001b[39mdeterministic\u001b[39;49m\u001b[39m\"\u001b[39;49m, deterministic,\n\u001b[0;32m   5950\u001b[0m       \u001b[39m\"\u001b[39;49m\u001b[39mpreserve_cardinality\u001b[39;49m\u001b[39m\"\u001b[39;49m, preserve_cardinality, \u001b[39m\"\u001b[39;49m\u001b[39mmetadata\u001b[39;49m\u001b[39m\"\u001b[39;49m, metadata)\n\u001b[0;32m   5951\u001b[0m     \u001b[39mreturn\u001b[39;00m _result\n\u001b[0;32m   5952\u001b[0m   \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "import gymnasium\n",
    "\n",
    "loaded_model = keras.models.load_model(\"trained_model.h5\",custom_objects={'my_loss_fn':DeepQLearning.my_loss_fn})\n",
    "\n",
    "sumObtainedRewards=0\n",
    "# simulate the learned policy for verification\n",
    "\n",
    "# create the environment, here you need to keep render_mode='rgb_array' since otherwise it will not generate the movie\n",
    "env = gym.make(\"CartPole-v1\", render_mode='rgb_array')\n",
    "# reset the environment\n",
    "(currentState, prob)=env.reset()\n",
    "\n",
    "# Wrapper for recording the video\n",
    "# https://gymnasium.farama.org/api/wrappers/misc_wrappers/#gymnasium.wrappers.RenderCollection\n",
    "# the name of the folder in which the video is stored is \"stored_video\"\n",
    "# length of the video in the number of simulation steps\n",
    "# if we do not specify the length, the video will be recorded until the end of the episode \n",
    "# that is, when terminalState becomes TRUE\n",
    "# just make sure that this parameter is smaller than the expected number of \n",
    "# time steps within an episode\n",
    "# for some reason this parameter does not produce the expected results, for smaller than 450 it gives OK results\n",
    "video_length=400\n",
    "# the step_trigger parameter is set to 1 in order to ensure that we record the video every step\n",
    "#env = gym.wrappers.RecordVideo(env, 'stored_video',step_trigger = lambda x: x == 1, video_length=video_length)\n",
    "env = gymnasium.wrappers.RecordVideo(env, 'stored_video', video_length=video_length)\n",
    "\n",
    "\n",
    "# since the initial state is not a terminal state, set this flag to false\n",
    "terminalState=False\n",
    "while not terminalState:\n",
    "    # get the Q-value (1 by 2 vector)\n",
    "    Qvalues=loaded_model.predict(currentState.reshape(1,4), verbose=0)\n",
    "    # select the action that gives the max Qvalue\n",
    "    action=np.random.choice(np.where(Qvalues[0,:]==np.max(Qvalues[0,:]))[0])\n",
    "    # if you want random actions for comparison\n",
    "    #action = env.action_space.sample()\n",
    "    # apply the action\n",
    "    (currentState, currentReward, terminalState,_,_) = env.step(action)\n",
    "    # sum the rewards\n",
    "    sumObtainedRewards+=currentReward\n",
    "    env.render()\n",
    "\n",
    "env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m terminalState\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m terminalState:\n\u001b[0;32m      5\u001b[0m     \u001b[39m# get the Q-value (1 by 2 vector)\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m     Qvalues\u001b[39m=\u001b[39mloaded_model\u001b[39m.\u001b[39;49mpredict(currentState\u001b[39m.\u001b[39;49mreshape(\u001b[39m1\u001b[39;49m,\u001b[39m4\u001b[39;49m), verbose\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[0;32m      7\u001b[0m     \u001b[39m# select the action that gives the max Qvalue\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     action\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mchoice(np\u001b[39m.\u001b[39mwhere(Qvalues[\u001b[39m0\u001b[39m,:]\u001b[39m==\u001b[39mnp\u001b[39m.\u001b[39mmax(Qvalues[\u001b[39m0\u001b[39m,:]))[\u001b[39m0\u001b[39m])\n",
      "File \u001b[1;32md:\\RL\\gym_version_new\\.venv\\Lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32md:\\RL\\gym_version_new\\.venv\\Lib\\site-packages\\keras\\engine\\training.py:2378\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   2376\u001b[0m callbacks\u001b[39m.\u001b[39mon_predict_begin()\n\u001b[0;32m   2377\u001b[0m batch_outputs \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 2378\u001b[0m \u001b[39mfor\u001b[39;00m _, iterator \u001b[39min\u001b[39;00m data_handler\u001b[39m.\u001b[39menumerate_epochs():  \u001b[39m# Single epoch.\u001b[39;00m\n\u001b[0;32m   2379\u001b[0m     \u001b[39mwith\u001b[39;00m data_handler\u001b[39m.\u001b[39mcatch_stop_iteration():\n\u001b[0;32m   2380\u001b[0m         \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m data_handler\u001b[39m.\u001b[39msteps():\n",
      "File \u001b[1;32md:\\RL\\gym_version_new\\.venv\\Lib\\site-packages\\keras\\engine\\data_adapter.py:1305\u001b[0m, in \u001b[0;36mDataHandler.enumerate_epochs\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1303\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[39;00m\n\u001b[0;32m   1304\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_truncate_execution_to_epoch():\n\u001b[1;32m-> 1305\u001b[0m     data_iterator \u001b[39m=\u001b[39m \u001b[39miter\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset)\n\u001b[0;32m   1306\u001b[0m     \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_initial_epoch, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_epochs):\n\u001b[0;32m   1307\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_insufficient_data:  \u001b[39m# Set by `catch_stop_iteration`.\u001b[39;00m\n",
      "File \u001b[1;32md:\\RL\\gym_version_new\\.venv\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:505\u001b[0m, in \u001b[0;36mDatasetV2.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    503\u001b[0m \u001b[39mif\u001b[39;00m context\u001b[39m.\u001b[39mexecuting_eagerly() \u001b[39mor\u001b[39;00m ops\u001b[39m.\u001b[39minside_function():\n\u001b[0;32m    504\u001b[0m   \u001b[39mwith\u001b[39;00m ops\u001b[39m.\u001b[39mcolocate_with(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variant_tensor):\n\u001b[1;32m--> 505\u001b[0m     \u001b[39mreturn\u001b[39;00m iterator_ops\u001b[39m.\u001b[39;49mOwnedIterator(\u001b[39mself\u001b[39;49m)\n\u001b[0;32m    506\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    507\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m`tf.data.Dataset` only supports Python-style \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    508\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39miteration in eager mode or within tf.function.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\RL\\gym_version_new\\.venv\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:713\u001b[0m, in \u001b[0;36mOwnedIterator.__init__\u001b[1;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[0;32m    709\u001b[0m   \u001b[39mif\u001b[39;00m (components \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m element_spec \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    710\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    711\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWhen `dataset` is provided, `element_spec` and `components` must \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    712\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mnot be specified.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 713\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_iterator(dataset)\n\u001b[0;32m    715\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_next_call_count \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[1;32md:\\RL\\gym_version_new\\.venv\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:752\u001b[0m, in \u001b[0;36mOwnedIterator._create_iterator\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    749\u001b[0m   \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(fulltype\u001b[39m.\u001b[39margs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39margs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39margs) \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(\n\u001b[0;32m    750\u001b[0m       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_output_types)\n\u001b[0;32m    751\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterator_resource\u001b[39m.\u001b[39mop\u001b[39m.\u001b[39mexperimental_set_type(fulltype)\n\u001b[1;32m--> 752\u001b[0m gen_dataset_ops\u001b[39m.\u001b[39;49mmake_iterator(ds_variant, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_iterator_resource)\n",
      "File \u001b[1;32md:\\RL\\gym_version_new\\.venv\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py:3439\u001b[0m, in \u001b[0;36mmake_iterator\u001b[1;34m(dataset, iterator, name)\u001b[0m\n\u001b[0;32m   3437\u001b[0m \u001b[39mif\u001b[39;00m tld\u001b[39m.\u001b[39mis_eager:\n\u001b[0;32m   3438\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3439\u001b[0m     _result \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_FastPathExecute(\n\u001b[0;32m   3440\u001b[0m       _ctx, \u001b[39m\"\u001b[39;49m\u001b[39mMakeIterator\u001b[39;49m\u001b[39m\"\u001b[39;49m, name, dataset, iterator)\n\u001b[0;32m   3441\u001b[0m     \u001b[39mreturn\u001b[39;00m _result\n\u001b[0;32m   3442\u001b[0m   \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode='human')\n",
    "env.reset()\n",
    "terminalState=False\n",
    "while not terminalState:\n",
    "    # get the Q-value (1 by 2 vector)\n",
    "    Qvalues=loaded_model.predict(currentState.reshape(1,4), verbose=0)\n",
    "    # select the action that gives the max Qvalue\n",
    "    action=np.random.choice(np.where(Qvalues[0,:]==np.max(Qvalues[0,:]))[0])\n",
    "    # if you want random actions for comparison\n",
    "    #action = env.action_space.sample()\n",
    "    # apply the action\n",
    "    (currentState, currentReward, terminalState,_,_) = env.step(action)\n",
    "    # sum the rewards\n",
    "    sumObtainedRewards+=currentReward\n",
    "    env.render()\n",
    "\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
