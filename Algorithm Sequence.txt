1. Value Iteration / Policy Iteration
2. Q-learning / SARSA
3. DQN
4. Policy Gradient
5. REINFORCE with baseline (subtracting policy gradient with a value function)
6. Actor Critic (similar to REINFORCE with baseline, but use bootstrapping instead of Monte Carlo for calculating expected return)
In 5, we call the value_function neural network at the end of each EPISODE because we compute advantage, loss, and update network after every EPISODE
In 6, we call the value_function neural network at the end of each STEP because we compute advantage, loss, and update network after every STEP
